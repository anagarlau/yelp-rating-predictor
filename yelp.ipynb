{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing the Yelp Comments on Berlin Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "#from nltk import word_tokenize\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Joseph ist ein ziemlich neues, israelisches Restaurant im modernen Design.  Es ist zwar nicht besonders kuschelig,  aber angenehm hell und hübsch eingerichtet mit schönen Lampen.  Der Service ist ausgesprochen freundlich, unkompliziert und aufmerksam, besonders Celina. Das Essen kam schnell,  sah sehr appetitlich aus und so schmeckte es auch. Die israelischen Starter,  Kleinigkeiten und Hauptspeisen haben uns sehr gut geschmeckt,  die Preise sind angemessen und für Berlin-Mitte nicht zu teuer. Wir waren nur etwas überrascht,  dass schon um 21.30 / 22h sehr laute Musik aufgelegt wurde, wir konnten uns fast nicht mehr unterhalten am 2er Tisch gegenüber.  Auf Anfrage wurde das etwas gedrosselt. Fazit: Wer israelische Küche mag, gutes Preis-Leistungs-Verhältnis möchte mit Blick auf die Friedrichstraße und netten Service ist hier sehr gut aufgehoben.\n"
     ]
    }
   ],
   "source": [
    "yelp = pd.read_csv('german_merged.csv')\n",
    "yelp.columns.values[0]=\"ID\"\n",
    "yelp = yelp.rename(columns={'Overall Rating':'Overall_Rating',\n",
    "                                      \"Total Reviews\":'Total_Reviews',\n",
    "                                      \"Restaurant Name\":\"Restaurant_Name\",\n",
    "                                     \"Price Range\":\"Price_Range\"})\n",
    "yelp.columns\n",
    "print(yelp['Comment'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "ID                   int64\nRestaurant_Name     object\nOverall_Rating     float64\nTotal_Reviews        int64\nSpecialty           object\nRegion              object\nPrice_Range         object\nAuthor              object\nComment             object\nRating               int64\nDate                object\ndtype: object"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    4.5\n1    4.5\n2    4.5\n3    4.5\n4    4.5\nName: Overall_Rating, dtype: float64"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['Comment'] = yelp['Comment'].astype(str)\n",
    "yelp['Overall_Rating'] = yelp['Overall_Rating'].astype(float)\n",
    "\n",
    "yelp.Overall_Rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   ID Restaurant_Name  Overall_Rating  Total_Reviews      Specialty Region  \\\n0   0          JOSEPH             4.5              6  Israelisch /   Mitte   \n1   1          JOSEPH             4.5              6  Israelisch /   Mitte   \n2   2          JOSEPH             4.5              6  Israelisch /   Mitte   \n3   3          JOSEPH             4.5              6  Israelisch /   Mitte   \n4   4          JOSEPH             4.5              6  Israelisch /   Mitte   \n\n  Price_Range      Author                                            Comment  \\\n0     Unknown  Dominik L.  Ich habe mir Bewertungen zu Restaurants auf Me...   \n1     Unknown   Regina D.  Das Joseph ist ein ziemlich neues, israelische...   \n2     Unknown    Heiko B.  Schalömchen, scheiße haben wir uns heute kosch...   \n3     Unknown   Torben B.  Essen war allenfalls ok, muss ich nicht nochma...   \n4     Unknown    Conny J.  Waren auf Empfehlung dort und wurden nicht ent...   \n\n   Rating        Date  \n0       5  26.10.2022  \n1       4   7.11.2019  \n2       5  18.10.2019  \n3       2    8.1.2020  \n4       5    3.9.2019  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Restaurant_Name</th>\n      <th>Overall_Rating</th>\n      <th>Total_Reviews</th>\n      <th>Specialty</th>\n      <th>Region</th>\n      <th>Price_Range</th>\n      <th>Author</th>\n      <th>Comment</th>\n      <th>Rating</th>\n      <th>Date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>JOSEPH</td>\n      <td>4.5</td>\n      <td>6</td>\n      <td>Israelisch /</td>\n      <td>Mitte</td>\n      <td>Unknown</td>\n      <td>Dominik L.</td>\n      <td>Ich habe mir Bewertungen zu Restaurants auf Me...</td>\n      <td>5</td>\n      <td>26.10.2022</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>JOSEPH</td>\n      <td>4.5</td>\n      <td>6</td>\n      <td>Israelisch /</td>\n      <td>Mitte</td>\n      <td>Unknown</td>\n      <td>Regina D.</td>\n      <td>Das Joseph ist ein ziemlich neues, israelische...</td>\n      <td>4</td>\n      <td>7.11.2019</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>JOSEPH</td>\n      <td>4.5</td>\n      <td>6</td>\n      <td>Israelisch /</td>\n      <td>Mitte</td>\n      <td>Unknown</td>\n      <td>Heiko B.</td>\n      <td>Schalömchen, scheiße haben wir uns heute kosch...</td>\n      <td>5</td>\n      <td>18.10.2019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>JOSEPH</td>\n      <td>4.5</td>\n      <td>6</td>\n      <td>Israelisch /</td>\n      <td>Mitte</td>\n      <td>Unknown</td>\n      <td>Torben B.</td>\n      <td>Essen war allenfalls ok, muss ich nicht nochma...</td>\n      <td>2</td>\n      <td>8.1.2020</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>JOSEPH</td>\n      <td>4.5</td>\n      <td>6</td>\n      <td>Israelisch /</td>\n      <td>Mitte</td>\n      <td>Unknown</td>\n      <td>Conny J.</td>\n      <td>Waren auf Empfehlung dort und wurden nicht ent...</td>\n      <td>5</td>\n      <td>3.9.2019</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Ich habe mir Bewertungen zu Restaurants auf Menulist angesehen, und dieses Restaurant hatte gute Kritiken, also habe ich es ausprobiert und es nicht bereut.menulist.menu/restaurant…'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.Comment.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Cleaning: Remove punctuation and set everything to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Remove unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    #Filter to allow only alphabets\n",
    "    #text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    #Fix &\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    \n",
    "    #Remove punctuations etc.\n",
    "    #text = re.sub(r'[?!.;:\",#@-]', '', text)\n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', '', text)\n",
    "    \n",
    "    #Remove Prices and numbers\n",
    "    text = re.sub(r'[0-9]+€|[0-9]+', '', text)\n",
    "\n",
    "    #Convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "    return text\n",
    "yelp['Comment'] = yelp.Comment.apply(lambda x: clean_text(x))\n",
    "#remove pattern \"x Fotos\" in Author - replace with Unknown\n",
    "#remove comments in english\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'ich habe mir bewertungen zu restaurants auf menulist angesehen, und dieses restaurant hatte gute kritiken, also habe ich es ausprobiert und es nicht bereut.menulist.menu/restaurant'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.Comment.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ich', 'habe', 'mir', 'bewertungen', 'zu', 'restaurants', 'auf', 'menulist', 'angesehen', 'und', 'dieses', 'restaurant', 'hatte', 'gute', 'kritiken', 'also', 'habe', 'ich', 'es', 'ausprobiert', 'und', 'es', 'nicht', 'bereut', 'menulist', 'menu', 'restaurant']\n"
     ]
    }
   ],
   "source": [
    "# using re\n",
    "# Explicitly tell Python that lower and upper case symbols are to be treated as the same symbol\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+',text)\n",
    "    return tokens\n",
    "yelp['Comment_tokenized'] = yelp['Comment'].apply (lambda x: tokenize(x.lower()))\n",
    "\n",
    "print(yelp.Comment_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "#dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"def lemmatizing(tokenized_text):\\n    text =  [wn.lemmatize(word) for word in tokenized_text]\\n    return text\\n\\nyelp['Comment_lemmatized'] = yelp['Comment_tokenized'].apply(lambda x: lemmatizing(x))\\n\""
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def lemmatizing(tokenized_text):\n",
    "    text =  [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "yelp['Comment_lemmatized'] = yelp['Comment_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation, Tokenize, remove stopwords and lemmatize all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([wn.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m yelp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComment_clean\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43myelp\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mComment\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mclean_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4771\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4661\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4662\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4663\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4666\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4667\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4668\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4669\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4670\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4769\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4770\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1105\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;66;03m# self.f is Callable\u001B[39;00m\n\u001B[1;32m-> 1105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1156\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1154\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1155\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[1;32m-> 1156\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1157\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1159\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1163\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1164\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn [30], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m yelp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComment_clean\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m yelp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComment\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mclean_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[1;32mIn [29], line 4\u001B[0m, in \u001B[0;36mclean_text\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      2\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([word\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string\u001B[38;5;241m.\u001B[39mpunctuation])\n\u001B[0;32m      3\u001B[0m tokens \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mW+\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m----> 4\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([wn\u001B[38;5;241m.\u001B[39mlemmatize(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "Cell \u001B[1;32mIn [29], line 4\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      2\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([word\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string\u001B[38;5;241m.\u001B[39mpunctuation])\n\u001B[0;32m      3\u001B[0m tokens \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mW+\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m----> 4\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([wn\u001B[38;5;241m.\u001B[39mlemmatize(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mstopwords\u001B[49m])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "\u001B[1;31mNameError\u001B[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "yelp['Comment_clean'] = yelp['Comment'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp[[\"ID\",\"Comment_clean\"]].to_csv(\"Yelp_Cleaned_Comments.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vecotrization- Process of encoding text as integers to create feature vectors, basically transform char to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_counts = count_vect.fit_transform(yelp['Comment'])\n",
    "\n",
    "# Peak into the words\n",
    "print(X_counts.shape)\n",
    "print(count_vect.get_feature_names()[189:204])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_df.columns = count_vect.get_feature_names()\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N - Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_ngram(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([wn.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['Comment_clean' ] = yelp['Comment'].apply(lambda x : clean_text_ngram(x))\n",
    "yelp.Comment_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "X_counts = ngram_vect.fit_transform(yelp['Comment_clean'])\n",
    "print(X_counts.shape)\n",
    "print(ngram_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df.columns = ngram_vect.get_feature_names()\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(yelp['Comment'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names()[1:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names_out()\n",
    "X_tfidf_df[['service','food','experience','place']]\n",
    "len(X_tfidf_df[X_tfidf_df['experience']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create new Features to best estimate the actual star rating given \n",
    "e.g. sentiment polarity & subjectivity with TextBlob,\n",
    "length of text field, \n",
    "percentage of characters that are punctuation in the text. \n",
    "percentage of characters that are capitalized\n",
    "\"\"\"\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(TextBlob)\n",
    "help(TextBlob.sentiment_assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmo = yelp['Comment_clean']\n",
    "\n",
    "def senti_score(text):\n",
    "    sentiment = []\n",
    "    sentiment_temp = []\n",
    "    result = []\n",
    "\n",
    "    for elem in text:\n",
    "        blob = TextBlob(elem)\n",
    "        for sentence in blob.sentences:\n",
    "            sentiment_temp.append(sentence.sentiment.polarity)\n",
    "        sentiment = Average(sentiment_temp)\n",
    "        result.append(sentiment)\n",
    "\n",
    "yelp['Sentiment'] = result\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis: longer comments tend to be correlated with more positive overall rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words from Word Frequency Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud\n",
    "\n",
    "# Import list of stopwards\n",
    "from wordcloud import STOPWORDS\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate frequency of every word from all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_freq(text):\n",
    "    #Will store the list of words\n",
    "    word_list = []\n",
    "\n",
    "    #Loop over all the comments and extract words into word_list\n",
    "    for tw_words in text.split():\n",
    "        word_list.extend(tw_words)\n",
    "\n",
    "    #Create word frequencies using word_list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "\n",
    "    #Print top 5 and bottom 5 words\n",
    "    word_freq[:10]\n",
    "    \n",
    "    return word_freq\n",
    "gen_freq(yelp.Comment.str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --> Problem: stopwords have highest frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Stopwords from frequency table\n",
    "word_freq = gen_freq(yelp.Comment.str)\n",
    "\n",
    "word_freq = word_freq.drop(labels=STOPWORDS, errors='ignore')\n",
    "word_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq[156:164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Generate word cloud\n",
    "wc = WordCloud(width=400, height=330, max_words=100, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic ML application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate even greater list with stopwords \n",
    "stop_words_manual =['a', 'about', 'above', 'after', 'again', 'against', 'all', 'also', 'am', 'an', 'and',\n",
    "              'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n",
    "              'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'com', 'could', \"couldn't\", 'did',\n",
    "              \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'else', 'ever',\n",
    "              'few', 'for', 'from', 'further', 'get', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having',\n",
    "              'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "              \"how's\", 'however', 'http', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it',\n",
    "              \"it's\", 'its', 'itself', 'just', 'k', \"let's\", 'like', 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "              'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'otherwise', 'ought', 'our', 'ours',\n",
    "              'ourselves', 'out', 'over', 'own', 'r', 'same', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\",\n",
    "              'should', \"shouldn't\", 'since', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs',\n",
    "              'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n",
    "              \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\",\n",
    "              'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    "              \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\",\n",
    "              'www', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "# Merge Manual list and wordcloud's stopwords list\n",
    "stop_words = list(STOPWORDS)+stop_words_manual\n",
    "\n",
    "# but remove duplicates\n",
    "stop_words = [*set(stop_words)]\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenrate Frequency of all words\n",
    "freq_clean = gen_freq(yelp.Comment.str).drop(labels=stop_words, errors='ignore')\n",
    "\n",
    "# how many different words do we still have \n",
    "# code to count number of entries in freq_clean\n",
    "\n",
    "# Get 100 rarest words only\n",
    "rare_100 =freq_clean[-100:]\n",
    "rare_100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables per restaurant comment from characteristics of Comment - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check whether a negation term is present in the text\n",
    "def any_neg(words):\n",
    "    for word in words:\n",
    "        if word in ['n', 'no', 'non', 'not'] or re.search(r\"\\wn't\", word):\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#Check whether one of the 100 rare words is present in the text\n",
    "def any_rare(words, rare_100):\n",
    "    for word in words:\n",
    "        if word in rare_100:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Check whether prompt words are present\n",
    "def is_question(words):\n",
    "    for word in words:\n",
    "        if word in ['when', 'what', 'how', 'why', 'who']:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of words in a comment\n",
    "yelp['word_count'] = yelp.Comment.str.split().apply(lambda x: len(x))\n",
    "#Negation present or not\n",
    "yelp['any_neg'] = yelp.Comment.str.split().apply(lambda x: any_neg(x))\n",
    "#Prompt present or not\n",
    "yelp['is_question'] = yelp.Comment.str.split().apply(lambda x: is_question(x))\n",
    "#Any of the most 100 rare words present or not\n",
    "yelp['any_rare'] = yelp.Comment.str.split().apply(lambda x: any_rare(x, rare_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yelp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic ML : Split dataset into predicting variables and predicted variable as well as into training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = yelp[['word_count', 'any_neg', 'any_rare', 'Price_Range', 'is_question','Overall_Rating']]\n",
    "y = yelp[['Rating']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Train an ML model for Text Classification\n",
    "\n",
    "##### Now that the dataset is ready, it is time to train a Machine Learning model on the same. You will be using a Naive Bayes classifier from sklearn which is a prominent python library used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Initialize GaussianNB classifier\n",
    "model = GaussianNB()\n",
    "#Fit the model on the train dataset\n",
    "model = model.fit(X_train, y_train)\n",
    "#Make predictions on the test datasetsyd\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred)*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
