{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1926,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing the Yelp Comments on Berlin Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1927,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1928,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "#from nltk import word_tokenize\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1929,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9721 entries, 0 to 9720\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   ID               9721 non-null   int32         \n",
      " 1   Restaurant_Name  9721 non-null   object        \n",
      " 2   Overall_Rating   9721 non-null   float64       \n",
      " 3   Total_Reviews    9721 non-null   int64         \n",
      " 4   Specialty        9721 non-null   object        \n",
      " 5   Region           9721 non-null   object        \n",
      " 6   Price_Range      9721 non-null   object        \n",
      " 7   Author           9721 non-null   object        \n",
      " 8   Comment          9721 non-null   object        \n",
      " 9   Rating           9721 non-null   int64         \n",
      " 10  Date             9721 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int32(1), int64(2), object(6)\n",
      "memory usage: 797.6+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ana\\AppData\\Local\\Temp\\ipykernel_4408\\745584405.py:11: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  yelp['Date'] = yelp['Date'].astype('datetime64[ns]',\"dd-MM-yyyy\")\n"
     ]
    }
   ],
   "source": [
    "yelp = pd.read_csv('1_scraping/intermediary_outputs/german_merged.csv')\n",
    "yelp.columns.values[0]=\"ID\"\n",
    "yelp = yelp.rename(columns={'Overall Rating':'Overall_Rating',\n",
    "                                      \"Total Reviews\":'Total_Reviews',\n",
    "                                      \"Restaurant Name\":\"Restaurant_Name\",\n",
    "                                     \"Price Range\":\"Price_Range\"})\n",
    "yelp.columns\n",
    "yelp['ID'] = yelp['ID'].astype(int)\n",
    "yelp['Comment'] = yelp['Comment'].astype(str)\n",
    "yelp['Overall_Rating'] = yelp['Overall_Rating'].astype(float)\n",
    "yelp['Date'] = yelp['Date'].astype('datetime64[ns]',\"dd-MM-yyyy\")\n",
    "yelp.dtypes\n",
    "print(yelp.info())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1938,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9706\n"
     ]
    }
   ],
   "source": [
    "yelp.head()\n",
    "# len(yelp)\n",
    "yelp = yelp.drop_duplicates(subset=['Comment'], inplace=False)\n",
    "print(len(yelp)) #15 duplicates found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Cleaning in Comment: Remove punctuation and set everything to lower case\n",
    "#### Basic Cleaning: replace \"Unknown\"/wrongly scraped names with None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1931,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL case\n",
      "ich habe mir bewertungen zu restaurants auf menulist angesehen und dieses restaurant hatte gute kritiken also habe ich es ausprobiert und es nicht bereut \n",
      "neue homepage\n",
      "Emoji case\n",
      "der wohl verrückteste kellner den ich je hatte aber eine einzigartige kneipe die sich hinter der stadtklause versteckt im kellergewölbe kann man auch noch sitzen günstiges deutsches essen und bier in einem eigentlich so touristisch überfluteten potsdamer platz ecke gerade bei den sonst so hohen bierpreisen in der umgebung des sony center liefert die stadtklause eine entspannung für den geldbeutel preisleistung stimmt\n",
      "super leckeres essen cooles atmosphäre  gutes bier leider keine ec karten oder kreditkarten aktzeptiert das ist ein bissle schade\n",
      "mit abstand das beste schnitzel weit und breit super nettes ambiente und tolles personal beim nächsten berlinbesuch wieder daumen hoch\n",
      "beste ramen die ich jemals gegessen habe ja es ist voll aber man wartet mi ich lange außer man entscheidet sich um 1900 ich werde wiederkommen\n",
      "Shoutouts\n",
      "1\n",
      "bayrische tapas in der baumreichen metzerstrasse nähe metzer eck die wirtsleute kommen aus bayern und sind sehr nett und gastfreundlich chef marcus ist gelernter brauer weihenstephan braut sein eigenes hervorragendes bier weizen und pils  sehr toll es gibt leberkäs weisswürste xöchts schweinsbraten und nachmittags von der chefin selbstgemachten käsekuchen draußen stehen schön schattige tische drinnen ist es aufgeräumt und klar vormieter war ein sushiladen der schöne breite marmortresen wurde übernommen gefällt mir sehr die wirtsleut und ihr schmankerlkonzept hat unbedingten geheimtippcharakter  komm ich jetzt öfter   danke für die empfehlung lieber\n",
      "2\n",
      "wir waren am 12 mai abends erstmals im casalot reservierung habe ich ganz kurzfristig telefonisch angefragt war aber kein problem zudem habe ich einen dailydealgutschein für vorspeisenmenü und grillplatte eingelöst was bei der bedienung ebenfalls problemlos akzeptiert wurde der service war die ganze zeit über sehr aufmerksam und überaus freundlich das essen war vielfältig und superlecker wir hatten nicht den eindruck dass wegen des dailydealgutscheins die portionen irgendwie mickrig waren die bedienung war bei uns angenehm zügig und ich kann die geäußerte kritik wegen langer wartezeiten nicht bestätigen obwohl das casalot fast vollständig besetzt war fazit seeeeehr lecker und toller service in verbindung mit gutscheinaktionen auch noch sehr günstig wobei aber auch die normalpreise keinen aus den schuhen hauen macht weiter so wir kommen gerne wieder\n",
      "3\n",
      "wenn es nicht ein gutes stück von meiner wohnung entfernt liegen würde dann käme ich wahrscheinlich in der woche mindestens einmal hierher ein preisleistungsverhältnis der besonderen sorte hier kriegt man gutes essen zu noch besseren preisen leckere pizzen ab 3 euro ich esse aber lieber die pasta mit käsesahne sauce die ist hier besonders lecker und kostengünstig ohne ende wenn es draußen warm ist kann man hervorragend auf der terrasse platz nehmen und hat einsicht auf die lehrter straße schmeckt wie beim italiener\n",
      "4\n",
      "wie schon von anderen geschrieben authentisch geil 2 worte thailändisches eisbein   wowowow bester thai der stadt    dokuwa\n",
      "Hashtags\n",
      "update 2018 das cocolo ist definitiv eines meiner stammrestaurants in berlin geworden mittlerweile war ich bestimmt schon zwei dutzend male dort betritt man das cocolo fühlt man sich sofort wieder wie in in einem guten ramen shop in tokyo man sollte wartezeit einplanen denn die plätze sind rar aber das warten lohnt sich ich hatte gyoza kimchi und das tonkotsu ramen es hat einfach großartig geschmeckt und meine freunde waren ebenfalls voll des lobes über das essen die brühe meines tonkotsu ramen war eher kräftig und wir hatten eine längere diskussion darüber ob man hier dem deutschen geschmack entgegegekommen ist oder ob wir auch in japan meist eine eher kräftige brühe hatten lange bevor wir eine einigung in dieser diskussion erziehlt hatten waren unsere schüsseln aber schon leergefuttert alles in allem eine echte empfehlung in berlin noch ein hinweis wie viele typische ramenshops in japan gibt es hier viel personal und wenig sitze man sollte so fair sein und die deutsche gewohnheit nach dem essen noch ein stündchen über seinem bier mit den freunden zu palavern einfach sein lassen und zügig in die nächste kneipe weiterziehen in japan ist das auch nicht anders einen guten shop erkennt man an den langen warteschlangen hat man dann seinen platz heißt es essen bezahlen wieder gehen das hat weniger mit gier nach geld zu tun als mit der tatsache daß der shop sonst nicht in die schwarzen zahlen kommt und wenn ein shop seinen profit verdient hat dan das cocolo\n",
      "mein erster abend im grill royal ich hatte viel gehört und war auf noch viel mehr gefasst schickis snobs wannabes etc stellen sie sich vor freitagabend im dezember 2010 nachdem wir kurz an der bar warten mussten da wir nicht reserviert hatten ich weiß ich weiß freitagabend wurden wir von der charmanten empfangsdame an unseren tisch geführt ratzfatz bekamen wir die abendkarte und wurden von der offensichtlich weinkundigen kellnerin bestens beraten das steak das der vorspeise tartar vom rinderfilet folgte war geradezu ein kulinarischer orgasmus lange habe ich kein derart gutes tbone steak  angus gegessen nun ja es hatte auch einen stolzen preis wie gewünscht war es medium rare dazu herrlich frisches gemüse und tolle pommes frites die beilagen sind übrigens frei wählbar auch der ganze hummer vom grill war das reinste ambrosia   und leider meiner begleitung vorbehalten aber ich weiß jetzt schon was ich nächstes mal esse zum dessert wurde uns ein entsprechender dessertwein serviert erst war ich skeptisch süße weine sind mir eigentlich ein greuel als ich aber den ersten schluck trank war es um mich geschehen als pinkelte mir ein engel auf die zunge es war einfach köstlich   und passend zum dessert nämlich halbflüssiger schokoladenkuchen kompliment an die küche wähnte ich den höhepunkt des abends doch eigentlich viel später zuhause mit meiner charmanten begleitung immerhin hab ich deren rechnung übernommen  zum publikum gelungene mischung hier trifft la bohme den promi sitzt tisch an tisch mit den immer wichtigen geschäftsleuten szenegängern und ottonormalverbrauchern die küche ist inspiriert kreativ und raffiniert in der zubereitung dies alles steht in einem guten nicht billigen  preisleistungsverhältnis abgerundet wird dies von der stets aufmerksamen jedoch nicht aufdringlichen bedienung und das hab ich jetzt nicht geschrieben weil es noch nen schnaps aufs haus gab ehrlich fazit ein gelungener wenngleich nicht billiger abend wir sind durch und durch befriedigt gegangen was ich in berliner restaurants dieses preissegments leider nicht oft sagen kann danke für den schönen abend ich komme wieder  \n"
     ]
    }
   ],
   "source": [
    "#remove pattern \"Unknown\" / \"x Fotos\" in Author - replace with None\n",
    "yelp['Author'] = yelp.Author.where((yelp.Author == 'Unknown') | ('Foto' in yelp.Author), None)\n",
    "yelp['Price_Range'] = yelp.Price_Range.where(yelp.Price_Range == 'Unknown', None)\n",
    "#print(yelp[yelp.ID==1114])\n",
    "\n",
    "def clean_text(text):\n",
    "    #remove random urls w/o http\n",
    "    text = re.sub('[\\w]+\\.[\\w]+\\/+[\\w]+','',text)\n",
    "    #remove URL with http\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text)\n",
    "    # add space after . to avoid word concatenation when user left no space after .\n",
    "    text = re.sub(r'(?<=[.,)!])(?=[^\\s])', r' ', text)\n",
    "    # Remove Emoji chars\n",
    "    emoticons = r'[\\W]+(?::|;|=)(?:-)?(?:\\)+|\\(|D|P)'\n",
    "    text = re.sub(emoticons, '' , text)\n",
    "    #remove hashtags but leave the content of a hashtag in text\n",
    "    text = re.sub(r\"([#]+)\", \"\", text)\n",
    "    #remove @name shoutouts +  weird shoutouts with space between @ and name\n",
    "    pattern_shoutout_one = r\"((\\w+|[^a-z])[@](\\s+\\w+|\\w+|.*))\"\n",
    "    text = re.sub(pattern_shoutout_one, \"\",text)\n",
    "    #Remove weird unicode characters such as U+2026\n",
    "    text = re.sub(r'[^\\x00-\\x7FäöüÄÖÜß]+', '', text)\n",
    "    # # remove hashtags and normal shoutouts with @\n",
    "    # pattern_hashtags_shoutouts = r\"([@#]\\w+)\"\n",
    "    # text = re.sub(pattern_hashtags_shoutouts,\"\" ,text)\n",
    "\n",
    "    #remove price patterns\n",
    "\n",
    "    #remove time patterns\n",
    "\n",
    "\n",
    "    #Filter to allow only alphabets\n",
    "    #text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    #Fix &\n",
    "    #text = re.sub(r'&amp;', '&', text)\n",
    "    \n",
    "    #Remove punctuations etc.\n",
    "   # text = re.sub(r'[?!.;:\",#@-]', ' ', text)\n",
    "    #text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', '', text)\n",
    "    \n",
    "    #Remove Prices and numbers\n",
    "    # text = re.sub(r'[0-9]+€|[0-9]+', '', text)\n",
    "\n",
    "    #Convert to lowercase to maintain consistency\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df = yelp[['ID', 'Comment']]\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "\n",
    "yelp['Comment'] = yelp.Comment.apply(lambda x: clean_text(x))\n",
    "print(\"URL case\")\n",
    "print(yelp.Comment[0])\n",
    "print(yelp.Comment[3562])\n",
    "print(\"Emoji case\")\n",
    "print(yelp.Comment[28])\n",
    "print(yelp.Comment[6])\n",
    "print(yelp.Comment[226])\n",
    "print(yelp.Comment[164])\n",
    "print(\"Shoutouts\")\n",
    "print(\"1\")\n",
    "print(yelp.Comment[278])#remove comments in english\n",
    "print(\"2\")\n",
    "print(yelp.Comment[2672])\n",
    "print(\"3\")\n",
    "print(yelp.Comment[6434])\n",
    "print(\"4\")\n",
    "print(yelp.Comment[2690])\n",
    "print(\"Hashtags\")\n",
    "print(yelp.Comment[1134])#remove comments in english\n",
    "print(yelp.Comment[4605])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1932,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'das als kleine bodega tapasrestaurant mit kleiner aussenterrasse für raucher ist eine toplocation für span küche sowohl die datteln im speckmantel wie die gambas al ajillo und die kartoffeln mit pikanter sauce sind spitze zu zweit bezahlten wir 60 euro samt 12 l rotwein 2 x datteln je 6 stück  2 hähnchen in honigsalsascharf hmmmm  garnelen in knoblauchsauce kroketten mit hühnchen sowie kartoffeln ca 10 stück frühlingskartoffeln wir finden das ist nicht das billigste aber seinen preis auf jeden fall wert wie heisst es in ebay weiter so und gerne wieder'"
     },
     "execution_count": 1932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.Comment.values[68]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1933,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ich', 'habe', 'mir', 'bewertungen', 'zu', 'restaurants', 'auf', 'menulist', 'angesehen', 'und', 'dieses', 'restaurant', 'hatte', 'gute', 'kritiken', 'also', 'habe', 'ich', 'es', 'ausprobiert', 'und', 'es', 'nicht', 'bereut', '']\n"
     ]
    }
   ],
   "source": [
    "# using re\n",
    "# Explicitly tell Python that lower and upper case symbols are to be treated as the same symbol\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+',text)\n",
    "    return tokens\n",
    "yelp['Comment_tokenized'] = yelp['Comment'].apply (lambda x: tokenize(x.lower()))\n",
    "\n",
    "print(yelp.Comment_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1934,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "#dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"def lemmatizing(tokenized_text):\\n    text =  [wn.lemmatize(word) for word in tokenized_text]\\n    return text\\n\\nyelp['Comment_lemmatized'] = yelp['Comment_tokenized'].apply(lambda x: lemmatizing(x))\\n\""
     },
     "execution_count": 1935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def lemmatizing(tokenized_text):\n",
    "    text =  [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "yelp['Comment_lemmatized'] = yelp['Comment_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation, Tokenize, remove stopwords and lemmatize all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1936,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([wn.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1937,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1937], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m yelp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComment_clean\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43myelp\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mComment\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mclean_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4771\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4661\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4662\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4663\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4666\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4667\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4668\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4669\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4670\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4769\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4770\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1105\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;66;03m# self.f is Callable\u001B[39;00m\n\u001B[1;32m-> 1105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1156\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1154\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1155\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[1;32m-> 1156\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1157\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1158\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1159\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1163\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1164\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1165\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn [1937], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m yelp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComment_clean\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m yelp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComment\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mclean_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[1;32mIn [1936], line 4\u001B[0m, in \u001B[0;36mclean_text\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      2\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([word\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string\u001B[38;5;241m.\u001B[39mpunctuation])\n\u001B[0;32m      3\u001B[0m tokens \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mW+\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m----> 4\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([wn\u001B[38;5;241m.\u001B[39mlemmatize(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "Cell \u001B[1;32mIn [1936], line 4\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      2\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([word\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m string\u001B[38;5;241m.\u001B[39mpunctuation])\n\u001B[0;32m      3\u001B[0m tokens \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mW+\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m----> 4\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([wn\u001B[38;5;241m.\u001B[39mlemmatize(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mstopwords\u001B[49m])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "\u001B[1;31mNameError\u001B[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "yelp['Comment_clean'] = yelp['Comment'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp[[\"ID\",\"Comment_clean\"]].to_csv(\"Yelp_Cleaned_Comments.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vecotrization- Process of encoding text as integers to create feature vectors, basically transform char to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_counts = count_vect.fit_transform(yelp['Comment'])\n",
    "\n",
    "# Peak into the words\n",
    "print(X_counts.shape)\n",
    "print(count_vect.get_feature_names()[189:204])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_df.columns = count_vect.get_feature_names()\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N - Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_ngram(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([wn.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['Comment_clean' ] = yelp['Comment'].apply(lambda x : clean_text_ngram(x))\n",
    "yelp.Comment_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "X_counts = ngram_vect.fit_transform(yelp['Comment_clean'])\n",
    "print(X_counts.shape)\n",
    "print(ngram_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df.columns = ngram_vect.get_feature_names()\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(yelp['Comment'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names()[1:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names_out()\n",
    "X_tfidf_df[['service','food','experience','place']]\n",
    "len(X_tfidf_df[X_tfidf_df['experience']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create new Features to best estimate the actual star rating given \n",
    "e.g. sentiment polarity & subjectivity with TextBlob,\n",
    "length of text field, \n",
    "percentage of characters that are punctuation in the text. \n",
    "percentage of characters that are capitalized\n",
    "\"\"\"\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(TextBlob)\n",
    "help(TextBlob.sentiment_assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmo = yelp['Comment_clean']\n",
    "\n",
    "def senti_score(text):\n",
    "    sentiment = []\n",
    "    sentiment_temp = []\n",
    "    result = []\n",
    "\n",
    "    for elem in text:\n",
    "        blob = TextBlob(elem)\n",
    "        for sentence in blob.sentences:\n",
    "            sentiment_temp.append(sentence.sentiment.polarity)\n",
    "        sentiment = Average(sentiment_temp)\n",
    "        result.append(sentiment)\n",
    "\n",
    "yelp['Sentiment'] = result\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis: longer comments tend to be correlated with more positive overall rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words from Word Frequency Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud\n",
    "\n",
    "# Import list of stopwards\n",
    "from wordcloud import STOPWORDS\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate frequency of every word from all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_freq(text):\n",
    "    #Will store the list of words\n",
    "    word_list = []\n",
    "\n",
    "    #Loop over all the comments and extract words into word_list\n",
    "    for tw_words in text.split():\n",
    "        word_list.extend(tw_words)\n",
    "\n",
    "    #Create word frequencies using word_list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "\n",
    "    #Print top 5 and bottom 5 words\n",
    "    word_freq[:10]\n",
    "    \n",
    "    return word_freq\n",
    "gen_freq(yelp.Comment.str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --> Problem: stopwords have highest frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Stopwords from frequency table\n",
    "word_freq = gen_freq(yelp.Comment.str)\n",
    "\n",
    "word_freq = word_freq.drop(labels=STOPWORDS, errors='ignore')\n",
    "word_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq[156:164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Generate word cloud\n",
    "wc = WordCloud(width=400, height=330, max_words=100, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic ML application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate even greater list with stopwords \n",
    "stop_words_manual =['a', 'about', 'above', 'after', 'again', 'against', 'all', 'also', 'am', 'an', 'and',\n",
    "              'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n",
    "              'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'com', 'could', \"couldn't\", 'did',\n",
    "              \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'else', 'ever',\n",
    "              'few', 'for', 'from', 'further', 'get', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having',\n",
    "              'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "              \"how's\", 'however', 'http', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it',\n",
    "              \"it's\", 'its', 'itself', 'just', 'k', \"let's\", 'like', 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "              'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'otherwise', 'ought', 'our', 'ours',\n",
    "              'ourselves', 'out', 'over', 'own', 'r', 'same', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\",\n",
    "              'should', \"shouldn't\", 'since', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs',\n",
    "              'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n",
    "              \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\",\n",
    "              'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    "              \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\",\n",
    "              'www', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "# Merge Manual list and wordcloud's stopwords list\n",
    "stop_words = list(STOPWORDS)+stop_words_manual\n",
    "\n",
    "# but remove duplicates\n",
    "stop_words = [*set(stop_words)]\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenrate Frequency of all words\n",
    "freq_clean = gen_freq(yelp.Comment.str).drop(labels=stop_words, errors='ignore')\n",
    "\n",
    "# how many different words do we still have \n",
    "# code to count number of entries in freq_clean\n",
    "\n",
    "# Get 100 rarest words only\n",
    "rare_100 =freq_clean[-100:]\n",
    "rare_100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables per restaurant comment from characteristics of Comment - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check whether a negation term is present in the text\n",
    "def any_neg(words):\n",
    "    for word in words:\n",
    "        if word in ['n', 'no', 'non', 'not'] or re.search(r\"\\wn't\", word):\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#Check whether one of the 100 rare words is present in the text\n",
    "def any_rare(words, rare_100):\n",
    "    for word in words:\n",
    "        if word in rare_100:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Check whether prompt words are present\n",
    "def is_question(words):\n",
    "    for word in words:\n",
    "        if word in ['when', 'what', 'how', 'why', 'who']:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of words in a comment\n",
    "yelp['word_count'] = yelp.Comment.str.split().apply(lambda x: len(x))\n",
    "#Negation present or not\n",
    "yelp['any_neg'] = yelp.Comment.str.split().apply(lambda x: any_neg(x))\n",
    "#Prompt present or not\n",
    "yelp['is_question'] = yelp.Comment.str.split().apply(lambda x: is_question(x))\n",
    "#Any of the most 100 rare words present or not\n",
    "yelp['any_rare'] = yelp.Comment.str.split().apply(lambda x: any_rare(x, rare_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yelp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic ML : Split dataset into predicting variables and predicted variable as well as into training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = yelp[['word_count', 'any_neg', 'any_rare', 'Price_Range', 'is_question','Overall_Rating']]\n",
    "y = yelp[['Rating']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Train an ML model for Text Classification\n",
    "\n",
    "##### Now that the dataset is ready, it is time to train a Machine Learning model on the same. You will be using a Naive Bayes classifier from sklearn which is a prominent python library used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Initialize GaussianNB classifier\n",
    "model = GaussianNB()\n",
    "#Fit the model on the train dataset\n",
    "model = model.fit(X_train, y_train)\n",
    "#Make predictions on the test datasetsyd\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred)*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
