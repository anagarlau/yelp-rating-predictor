{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing the Yelp Comments on Berlin Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 593.2 kB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     -------------------------------------- 78.5/78.5 kB 545.8 kB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     -------------------------------------- 96.6/96.6 kB 616.8 kB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp310-cp310-win_amd64.whl (267 kB)\n",
      "     ------------------------------------ 267.7/267.7 kB 587.8 kB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------ 298.0/298.0 kB 613.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\ana\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.7 regex-2022.10.31 tqdm-4.64.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: C:\\Users\\ana\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "#from nltk import word_tokenize\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Joseph ist ein ziemlich neues, israelisches Restaurant im modernen Design.  Es ist zwar nicht besonders kuschelig,  aber angenehm hell und hübsch eingerichtet mit schönen Lampen.  Der Service ist ausgesprochen freundlich, unkompliziert und aufmerksam, besonders Celina. Das Essen kam schnell,  sah sehr appetitlich aus und so schmeckte es auch. Die israelischen Starter,  Kleinigkeiten und Hauptspeisen haben uns sehr gut geschmeckt,  die Preise sind angemessen und für Berlin-Mitte nicht zu teuer. Wir waren nur etwas überrascht,  dass schon um 21.30 / 22h sehr laute Musik aufgelegt wurde, wir konnten uns fast nicht mehr unterhalten am 2er Tisch gegenüber.  Auf Anfrage wurde das etwas gedrosselt. Fazit: Wer israelische Küche mag, gutes Preis-Leistungs-Verhältnis möchte mit Blick auf die Friedrichstraße und netten Service ist hier sehr gut aufgehoben.\n"
     ]
    }
   ],
   "source": [
    "yelp = pd.read_csv('german_merged.csv')\n",
    "yelp.columns.values[0]=\"ID\"\n",
    "yelp = yelp.rename(columns={'Overall Rating':'Overall_Rating',\n",
    "                                      \"Total Reviews\":'Total_Reviews',\n",
    "                                      \"Restaurant Name\":\"Restaurant_Name\",\n",
    "                                     \"Price Range\":\"Price_Range\"})\n",
    "yelp.columns\n",
    "print(yelp['Comment'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yelp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43myelp\u001b[49m\u001b[38;5;241m.\u001b[39mdtypes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yelp' is not defined"
     ]
    }
   ],
   "source": [
    "yelp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yelp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m yelp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43myelp\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      2\u001b[0m yelp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m yelp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      4\u001b[0m yelp\u001b[38;5;241m.\u001b[39mOverall_Rating\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yelp' is not defined"
     ]
    }
   ],
   "source": [
    "yelp['Comment'] = yelp['Comment'].astype(str)\n",
    "yelp['Overall_Rating'] = yelp['Overall_Rating'].astype(int)\n",
    "\n",
    "yelp.Overall_Rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yelp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43myelp\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yelp' is not defined"
     ]
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yelp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43myelp\u001b[49m\u001b[38;5;241m.\u001b[39mComment\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yelp' is not defined"
     ]
    }
   ],
   "source": [
    "yelp.Comment.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Cleaning: Remove punctuation and set everything to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yelp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 21\u001b[0m yelp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43myelp\u001b[49m\u001b[38;5;241m.\u001b[39mComment\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: clean_text(x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yelp' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    #Remove unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    #Filter to allow only alphabets\n",
    "    #text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    #Fix &\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    \n",
    "    #Remove punctuations etc.\n",
    "    #text = re.sub(r'[?!.;:\",#@-]', '', text)\n",
    "    text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', '', text)\n",
    "    \n",
    "    #Remove Prices and numbers\n",
    "    text = re.sub(r'[0-9]+€|[0-9]+', '', text)\n",
    "\n",
    "    #Convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "    return text\n",
    "yelp['Comment'] = yelp.Comment.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yelp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43myelp\u001b[49m\u001b[38;5;241m.\u001b[39mComment\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yelp' is not defined"
     ]
    }
   ],
   "source": [
    "yelp.Comment.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using re\n",
    "# Explicitly tell Python that lower and upper case symbols are to be treated as the same symbol\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+',text)\n",
    "    return tokens\n",
    "yelp['Comment_tokenized'] = yelp['Comment'].apply (lambda x: tokenize(x.lower()))\n",
    "\n",
    "print(yelp.Comment_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "#dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def lemmatizing(tokenized_text):\n",
    "    text =  [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "yelp['Comment_lemmatized'] = yelp['Comment_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation, Tokenize, remove stopwords and lemmatize all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([wn.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yelp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m yelp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComment_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43myelp\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: clean_text(x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yelp' is not defined"
     ]
    }
   ],
   "source": [
    "yelp['Comment_clean'] = yelp['Comment'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yelp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43myelp\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComment_clean\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYelp_Cleaned_Comments.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yelp' is not defined"
     ]
    }
   ],
   "source": [
    "yelp[[\"ID\",\"Comment_clean\"]].to_csv(\"Yelp_Cleaned_Comments.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vecotrization- Process of encoding text as integers to create feature vectors, basically transform char to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      3\u001b[0m count_vect \u001b[38;5;241m=\u001b[39m CountVectorizer(analyzer\u001b[38;5;241m=\u001b[39mclean_text)\n\u001b[0;32m      4\u001b[0m X_counts \u001b[38;5;241m=\u001b[39m count_vect\u001b[38;5;241m.\u001b[39mfit_transform(yelp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_counts = count_vect.fit_transform(yelp['Comment'])\n",
    "\n",
    "# Peak into the words\n",
    "print(X_counts.shape)\n",
    "print(count_vect.get_feature_names()[189:204])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX_counts\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_counts' is not defined"
     ]
    }
   ],
   "source": [
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_counts_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(X_counts\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[0;32m      2\u001b[0m X_counts_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_counts_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m \u001b[43mcount_vect\u001b[49m\u001b[38;5;241m.\u001b[39mget_feature_names()\n\u001b[0;32m      2\u001b[0m X_counts_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'count_vect' is not defined"
     ]
    }
   ],
   "source": [
    "X_counts_df.columns = count_vect.get_feature_names()\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N - Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_ngram(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([wn.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['Comment_clean' ] = yelp['Comment'].apply(lambda x : clean_text_ngram(x))\n",
    "yelp.Comment_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "X_counts = ngram_vect.fit_transform(yelp['Comment_clean'])\n",
    "print(X_counts.shape)\n",
    "print(ngram_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df.columns = ngram_vect.get_feature_names()\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(yelp['Comment'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names()[1:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names_out()\n",
    "X_tfidf_df[['service','food','experience','place']]\n",
    "len(X_tfidf_df[X_tfidf_df['experience']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCreate new Features to best estimate the actual star rating given \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03me.g. sentiment polarity & subjectivity with TextBlob,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mpercentage of characters that are capitalized\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create new Features to best estimate the actual star rating given \n",
    "e.g. sentiment polarity & subjectivity with TextBlob,\n",
    "length of text field, \n",
    "percentage of characters that are punctuation in the text. \n",
    "percentage of characters that are capitalized\n",
    "\"\"\"\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextBlob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#dir(TextBlob)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m help(\u001b[43mTextBlob\u001b[49m\u001b[38;5;241m.\u001b[39msentiment_assessments)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextBlob' is not defined"
     ]
    }
   ],
   "source": [
    "#dir(TextBlob)\n",
    "help(TextBlob.sentiment_assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmo = yelp['Comment_clean']\n",
    "\n",
    "def senti_score(text):\n",
    "    sentiment = []\n",
    "    sentiment_temp = []\n",
    "    result = []\n",
    "\n",
    "    for elem in text:\n",
    "        blob = TextBlob(elem)\n",
    "        for sentence in blob.sentences:\n",
    "            sentiment_temp.append(sentence.sentiment.polarity)\n",
    "        sentiment = Average(sentiment_temp)\n",
    "        result.append(sentiment)\n",
    "\n",
    "yelp['Sentiment'] = result\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis: longer comments tend to be correlated with more positive overall rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words from Word Frequency Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud\n",
    "\n",
    "# Import list of stopwards\n",
    "from wordcloud import STOPWORDS\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate frequency of every word from all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_freq(text):\n",
    "    #Will store the list of words\n",
    "    word_list = []\n",
    "\n",
    "    #Loop over all the comments and extract words into word_list\n",
    "    for tw_words in text.split():\n",
    "        word_list.extend(tw_words)\n",
    "\n",
    "    #Create word frequencies using word_list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "\n",
    "    #Print top 5 and bottom 5 words\n",
    "    word_freq[:10]\n",
    "    \n",
    "    return word_freq\n",
    "gen_freq(yelp.Comment.str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --> Problem: stopwords have highest frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Stopwords from frequency table\n",
    "word_freq = gen_freq(yelp.Comment.str)\n",
    "\n",
    "word_freq = word_freq.drop(labels=STOPWORDS, errors='ignore')\n",
    "word_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq[156:164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Generate word cloud\n",
    "wc = WordCloud(width=400, height=330, max_words=100, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic ML application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate even greater list with stopwords \n",
    "stop_words_manual =['a', 'about', 'above', 'after', 'again', 'against', 'all', 'also', 'am', 'an', 'and',\n",
    "              'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n",
    "              'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'com', 'could', \"couldn't\", 'did',\n",
    "              \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'else', 'ever',\n",
    "              'few', 'for', 'from', 'further', 'get', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having',\n",
    "              'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "              \"how's\", 'however', 'http', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it',\n",
    "              \"it's\", 'its', 'itself', 'just', 'k', \"let's\", 'like', 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "              'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'otherwise', 'ought', 'our', 'ours',\n",
    "              'ourselves', 'out', 'over', 'own', 'r', 'same', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\",\n",
    "              'should', \"shouldn't\", 'since', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs',\n",
    "              'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n",
    "              \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\",\n",
    "              'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    "              \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\",\n",
    "              'www', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "# Merge Manual list and wordcloud's stopwords list\n",
    "stop_words = list(STOPWORDS)+stop_words_manual\n",
    "\n",
    "# but remove duplicates\n",
    "stop_words = [*set(stop_words)]\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenrate Frequency of all words\n",
    "freq_clean = gen_freq(yelp.Comment.str).drop(labels=stop_words, errors='ignore')\n",
    "\n",
    "# how many different words do we still have \n",
    "# code to count number of entries in freq_clean\n",
    "\n",
    "# Get 100 rarest words only\n",
    "rare_100 =freq_clean[-100:]\n",
    "rare_100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables per restaurant comment from characteristics of Comment - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check whether a negation term is present in the text\n",
    "def any_neg(words):\n",
    "    for word in words:\n",
    "        if word in ['n', 'no', 'non', 'not'] or re.search(r\"\\wn't\", word):\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#Check whether one of the 100 rare words is present in the text\n",
    "def any_rare(words, rare_100):\n",
    "    for word in words:\n",
    "        if word in rare_100:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Check whether prompt words are present\n",
    "def is_question(words):\n",
    "    for word in words:\n",
    "        if word in ['when', 'what', 'how', 'why', 'who']:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of words in a comment\n",
    "yelp['word_count'] = yelp.Comment.str.split().apply(lambda x: len(x))\n",
    "#Negation present or not\n",
    "yelp['any_neg'] = yelp.Comment.str.split().apply(lambda x: any_neg(x))\n",
    "#Prompt present or not\n",
    "yelp['is_question'] = yelp.Comment.str.split().apply(lambda x: is_question(x))\n",
    "#Any of the most 100 rare words present or not\n",
    "yelp['any_rare'] = yelp.Comment.str.split().apply(lambda x: any_rare(x, rare_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yelp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic ML : Split dataset into predicting variables and predicted variable as well as into training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m yelp[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124many_neg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124many_rare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice_Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_question\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m yelp[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = yelp[['word_count', 'any_neg', 'any_rare', 'Price_Range', 'is_question','Overall_Rating']]\n",
    "y = yelp[['Rating']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Train an ML model for Text Classification\n",
    "\n",
    "##### Now that the dataset is ready, it is time to train a Machine Learning model on the same. You will be using a Naive Bayes classifier from sklearn which is a prominent python library used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Initialize GaussianNB classifier\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m GaussianNB()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Initialize GaussianNB classifier\n",
    "model = GaussianNB()\n",
    "#Fit the model on the train dataset\n",
    "model = model.fit(X_train, y_train)\n",
    "#Make predictions on the test datasetsyd\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred)*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
